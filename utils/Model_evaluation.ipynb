{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cabroderick/ML-AM-MQP/blob/main/Model_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3amtZgDq2EN6",
    "outputId": "1c5ce28f-8b73-4e3f-c197-f84438ee2b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n",
      "Found existing installation: keras 2.7.0\n",
      "Uninstalling keras-2.7.0:\n",
      "  Successfully uninstalled keras-2.7.0\n",
      "\u001b[33mWARNING: Skipping keras-nightly as it is not installed.\u001b[0m\n",
      "Found existing installation: Keras-Preprocessing 1.1.2\n",
      "Uninstalling Keras-Preprocessing-1.1.2:\n",
      "  Successfully uninstalled Keras-Preprocessing-1.1.2\n",
      "Found existing installation: keras-vis 0.4.1\n",
      "Uninstalling keras-vis-0.4.1:\n",
      "  Successfully uninstalled keras-vis-0.4.1\n",
      "Found existing installation: tensorflow 2.7.0\n",
      "Uninstalling tensorflow-2.7.0:\n",
      "  Successfully uninstalled tensorflow-2.7.0\n",
      "Found existing installation: h5py 3.1.0\n",
      "Uninstalling h5py-3.1.0:\n",
      "  Successfully uninstalled h5py-3.1.0\n",
      "Collecting tensorflow==1.13.1\n",
      "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 92.6 MB 72 kB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 6.1 MB/s \n",
      "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
      "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 49.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.43.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.4.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.17.3)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
      "\u001b[K     |████████████████████████████████| 367 kB 57.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.12.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.37.1)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 60.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.10.0.2)\n",
      "Collecting mock>=2.0.0\n",
      "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n",
      "Installing collected packages: mock, h5py, tensorflow-estimator, tensorboard, keras-preprocessing, keras-applications, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\u001b[0m\n",
      "Successfully installed h5py-3.6.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n",
      "Collecting keras==2.0.8\n",
      "  Downloading Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n",
      "\u001b[K     |████████████████████████████████| 276 kB 5.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (1.19.5)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.0.8\n",
      "Collecting h5py==2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 4.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
      "Installing collected packages: h5py\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.6.0\n",
      "    Uninstalling h5py-3.6.0:\n",
      "      Successfully uninstalled h5py-3.6.0\n",
      "Successfully installed h5py-2.10.0\n",
      "/root\n",
      "Cloning into 'Mask_RCNN'...\n",
      "remote: Enumerating objects: 956, done.\u001b[K\n",
      "remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\u001b[K\n",
      "Receiving objects: 100% (956/956), 125.23 MiB | 34.08 MiB/s, done.\n",
      "Resolving deltas: 100% (565/565), done.\n",
      "/root/Mask_RCNN\n",
      "WARNING:root:Fail load requirements file, so using default ones.\n",
      "/usr/local/lib/python3.7/dist-packages/setuptools/dist.py:700: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "  % (opt, underscore_opt))\n",
      "/usr/local/lib/python3.7/dist-packages/setuptools/dist.py:700: UserWarning: Usage of dash-separated 'license-file' will not be supported in future versions. Please use the underscore name 'license_file' instead\n",
      "  % (opt, underscore_opt))\n",
      "/usr/local/lib/python3.7/dist-packages/setuptools/dist.py:700: UserWarning: Usage of dash-separated 'requirements-file' will not be supported in future versions. Please use the underscore name 'requirements_file' instead\n",
      "  % (opt, underscore_opt))\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "creating mask_rcnn.egg-info\n",
      "writing mask_rcnn.egg-info/PKG-INFO\n",
      "writing dependency_links to mask_rcnn.egg-info/dependency_links.txt\n",
      "writing top-level names to mask_rcnn.egg-info/top_level.txt\n",
      "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "creating build/lib/mrcnn\n",
      "copying mrcnn/model.py -> build/lib/mrcnn\n",
      "copying mrcnn/parallel_model.py -> build/lib/mrcnn\n",
      "copying mrcnn/utils.py -> build/lib/mrcnn\n",
      "copying mrcnn/__init__.py -> build/lib/mrcnn\n",
      "copying mrcnn/visualize.py -> build/lib/mrcnn\n",
      "copying mrcnn/config.py -> build/lib/mrcnn\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/mrcnn\n",
      "copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
      "copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
      "copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
      "copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
      "copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
      "copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
      "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying mask_rcnn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying mask_rcnn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying mask_rcnn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying mask_rcnn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating dist\n",
      "creating 'dist/mask_rcnn-2.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing mask_rcnn-2.1-py3.7.egg\n",
      "Copying mask_rcnn-2.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
      "Adding mask-rcnn 2.1 to easy-install.pth file\n",
      "\n",
      "Installed /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg\n",
      "Processing dependencies for mask-rcnn==2.1\n",
      "Finished processing dependencies for mask-rcnn==2.1\n",
      "Name: mask-rcnn\n",
      "Version: 2.1\n",
      "Summary: Mask R-CNN for object detection and instance segmentation\n",
      "Home-page: https://github.com/matterport/Mask_RCNN\n",
      "Author: Matterport\n",
      "Author-email: waleed.abdulla@gmail.com\n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg\n",
      "Requires: \n",
      "Required-by: \n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python --version\n",
    "\n",
    "# uninstall improper package versions\n",
    "!pip uninstall keras -y\n",
    "!pip uninstall keras-nightly -y\n",
    "!pip uninstall keras-Preprocessing -y\n",
    "!pip uninstall keras-vis -y\n",
    "!pip uninstall tensorflow -y\n",
    "!pip uninstall h5py -y\n",
    "\n",
    "# reinstall with proper versions\n",
    "!pip install tensorflow==1.13.1\n",
    "!pip install keras==2.0.8\n",
    "!pip install h5py==2.10.0\n",
    "\n",
    "# import mask rcnn and set up\n",
    "%cd\n",
    "!git clone https://github.com/matterport/Mask_RCNN.git\n",
    "%cd Mask_RCNN/\n",
    "!python setup.py install\n",
    "!pip show mask-rcnn\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWfzAaVh25sU",
    "outputId": "d688a671-af84-4155-bbdc-8003f35c049f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "from mrcnn.model import load_image_gt\n",
    "from mrcnn import utils\n",
    "from sklearn import metrics\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from mrcnn.utils import compute_recall\n",
    "\n",
    "#%%\n",
    "\n",
    "# specify paths for prediction\n",
    "WEIGHTS_PATH = '/content/drive/MyDrive/MQP/custom_mrcnn.h5'\n",
    "TEST_IMGS = []\n",
    "CLASS_NAMES = ['gas entrapment porosity', 'lack of fusion porosity', 'keyhole porosity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AWoF-fr83B1A"
   },
   "outputs": [],
   "source": [
    "\n",
    "# configure inference model\n",
    "class InferenceConfig(Config):\n",
    "    NAME = 'inference'\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 4\n",
    "    MAGE_RESIZE_MODE = \"square\"\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    IMAGE_MIN_SCALE = 2.0\n",
    "\n",
    "cfg = InferenceConfig()\n",
    "\n",
    "model = MaskRCNN(mode='inference',\n",
    "                 config=cfg, model_dir='./')\n",
    "\n",
    "model.load_weights(filepath=WEIGHTS_PATH, by_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2IsIR1te3bwq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TestDataset(utils.Dataset):\n",
    "    image_list = []\n",
    "    # define constants\n",
    "    BASE_IMAGES_DIR = '/content/drive/MyDrive/MQP/Data/Eval/Images/' # directory where all images can be found\n",
    "    BASE_ANNOTATIONS_DIR = '/content/drive/MyDrive/MQP/Data/Eval/Labels/' # directory where all images labels can be found\n",
    "    IMAGES_DIRS = ['G0/', 'G9/', 'H0/', 'H4/', 'H5/', 'H6/', 'H8/', 'H9/', 'J0/', 'J3/', 'J4/', 'J7/',\n",
    "                 'J8/', 'K0/', 'Q0/', 'Q3/', 'Q5/', 'Q9/', 'R2/', 'R6/', 'R7/'] # list of directories where images are contained\n",
    "    ANNOTATIONS_DIRS = ['Labeled G0/', 'Labeled G9/', 'Labeled H0/', 'Labeled H4/', 'Labeled H5/', 'Labeled H6/',\n",
    "                      'Labeled H8/', 'Labeled H9/', 'Labeled J0/', 'Labeled J3/', 'Labeled J4/',\n",
    "                      'Labeled J7/', 'Labeled J8/', 'Labeled K0/', 'Labeled Q0/', 'Labeled Q3/', 'Labeled Q5/',\n",
    "                      'Labeled Q9/', 'Labeled R2/', 'Labeled R6/', 'Labeled R7/'] # corresponding list of directories where annotations are contained\n",
    "\n",
    "    CLASSES = ['lack of fusion porosity', 'keyhole porosity', 'other'] # all annotation classes\n",
    "\n",
    "    def load_dataset(self):\n",
    "        image_paths = []\n",
    "        annotation_paths = []\n",
    "        image_ids = []\n",
    "\n",
    "        for i in range(len(self.IMAGES_DIRS)):\n",
    "            image_paths.append([])\n",
    "            annotation_paths.append([])\n",
    "            image_ids.append([])\n",
    "            i_dir = self.BASE_IMAGES_DIR + self.IMAGES_DIRS[i]\n",
    "            a_dir = self.BASE_ANNOTATIONS_DIR + self.ANNOTATIONS_DIRS[i]\n",
    "            for file in os.listdir(i_dir):\n",
    "                i_id = file[:-4]\n",
    "                image_ids[i].append(i_id)\n",
    "                image_paths[i].append(i_dir+i_id+'.tif')\n",
    "                if \"20X_YZ\" not in i_id:\n",
    "                  annotation_paths[i].append(a_dir+i_id+'_20X_YZ.json')\n",
    "                else:\n",
    "                  annotation_paths[i].append(a_dir+i_id+'.json')\n",
    "\n",
    "        if (len(image_paths) != len(annotation_paths)): # raise exception if mismatch betwaeen number of images and annotations\n",
    "            raise(ValueError('Number of images and annotations must be equal'))\n",
    "        # configure dataset\n",
    "        for i in range(len(self.CLASSES)):\n",
    "            self.add_class('dataset', i+1, self.CLASSES[i]) # add classes to model\n",
    "\n",
    "        # add images and annotations to dataset, ensuring an even distribution\n",
    "        for i in range(len(image_paths)):\n",
    "            images = len(image_paths[i])\n",
    "            for j in range(images):\n",
    "              image_id = image_ids[i][j]\n",
    "              image_path = image_paths[i][j]\n",
    "              annotation_path = annotation_paths[i][j]\n",
    "\n",
    "              mask, class_ids = self.extract_mask(image_path, annotation_path)\n",
    "\n",
    "              if len(mask) != 0: # skip images with no annotations\n",
    "                self.image_list.append(image_path.replace(self.BASE_IMAGES_DIR, \"\"))\n",
    "                self.add_image('dataset',\n",
    "                              image_id=image_id,\n",
    "                              path=image_path,\n",
    "                              mask=mask,\n",
    "                              class_ids=class_ids)\n",
    "    '''\n",
    "    Extracts a mask from an image\n",
    "    image_id: The image id to extract the mask from\n",
    "    Returns a mask and a corresponding list of class ids\n",
    "    '''\n",
    "    def load_mask(self, image_id):\n",
    "\n",
    "        info = self.image_info[image_id] # extract image info from data added earlier\n",
    "        mask = info['mask']\n",
    "        class_ids = info['class_ids']\n",
    "\n",
    "        return mask, class_ids\n",
    "\n",
    "    def extract_mask(self, image_path, annotation_path):\n",
    "      if not os.path.exists(annotation_path): # if the annotation path is not found, it is named differently than its source image\n",
    "          annotation_path = annotation_path[:-5] + '_20X_YZ.json'\n",
    "      if not os.path.exists(annotation_path): # if the annotation path is not found, it is named differently than its source image\n",
    "          annotation_path = annotation_path.replace('_20X_YZ', \"\")\n",
    "\n",
    "      print(image_path, annotation_path)\n",
    "\n",
    "      f_ann = open(annotation_path,)\n",
    "      annotation_json = json.load(f_ann)\n",
    "\n",
    "      if not annotation_json['shapes']: # if there are no annotations to be extracted\n",
    "          return [], [] # empty list return values will be ignored and thus image is ignored\n",
    "\n",
    "      class_ids = []\n",
    "      image = cv2.imread(image_path)\n",
    "      height = image.shape[0]\n",
    "      width = image.shape[1]\n",
    "\n",
    "      annotation_list = []\n",
    "      [annotation_list.append(shape) for shape in annotation_json['shapes'] if shape['shape_type'] =='rectangle'\n",
    "      and self.normalize_classname(shape['label']) != 'gas entrapment porosity'] # get annotations in a list\n",
    "      mask = np.zeros([height, width, len(annotation_list)], dtype='uint8') # initialize array of masks for each bounding box\n",
    "\n",
    "      for i in range(len(annotation_list)):\n",
    "        a = annotation_list[i]\n",
    "\n",
    "        # extract row and col data and crop image to annotation size\n",
    "        col_min, col_max = int(min(a['points'][0][0], a['points'][1][0])), int(max(a['points'][0][0], a['points'][1][0]))\n",
    "        row_min, row_max = int(min(a['points'][0][1], a['points'][1][1])), int(max(a['points'][0][1], a['points'][1][1]))\n",
    "        col_min, col_max, row_min, row_max = self.normalize_dimensions(col_min, col_max, row_min, row_max)\n",
    "        cropped_img = image[row_min:row_max, col_min:col_max]  # crop image to size of bounding box\n",
    "        cropped_img_gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n",
    "        edged = cv2.Canny(cropped_img_gray, 30, 200)\n",
    "\n",
    "        # apply contour to image and fill\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
    "        dilated = cv2.dilate(edged, kernel)\n",
    "        contours, hierarchy = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        polygon = np.zeros(cropped_img.shape)\n",
    "        color = [255, 255, 255]\n",
    "        cv2.fillPoly(polygon, contours, color)\n",
    "\n",
    "        # normalize polygon to all boolean values and insert into mask\n",
    "        polygon_bool = np.alltrue(polygon == color, axis=2)\n",
    "        mask[row_min:row_max, col_min:col_max, i] = polygon_bool\n",
    "\n",
    "        # draw contour and mask\n",
    "        # cv2.drawContours(edged, contours, -1, (0, 255, 0), 1)\n",
    "        # imS = cv2.resize(edged, (512, 512))\n",
    "        # cv2.imshow('Contours', imS)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.imshow('Polygon', cv2.resize(polygon, (512, 512)))\n",
    "        # cv2.waitKey(0)\n",
    "\n",
    "        # extract class id and append to list\n",
    "        class_label = self.normalize_classname(a['label'])\n",
    "        class_id = self.CLASSES.index(class_label)\n",
    "        class_ids.append(class_id)\n",
    "\n",
    "      return mask.astype(np.bool), np.array(class_ids, dtype=np.int32)\n",
    "\n",
    "    def normalize_classname(self, class_name): # normalize the class name to one used by the model\n",
    "      class_name = class_name.lower() # remove capitalization\n",
    "      class_name = class_name.strip() # remove leading and trailing whitespace\n",
    "      classes_dict = { # dictionary containing all class names used in labels and their appropriate model class name\n",
    "        'gas entrapment porosity': 'gas entrapment porosity',\n",
    "        'keyhole porosity': 'keyhole porosity',\n",
    "        'lack of fusion porosity': 'lack of fusion porosity',\n",
    "        'fusion porosity': 'lack of fusion porosity',\n",
    "        'gas porosity': 'gas entrapment porosity',\n",
    "        'lack-of-fusion': 'lack of fusion porosity',\n",
    "        'keyhole': 'keyhole porosity',\n",
    "        'other': 'other',\n",
    "        'lack of fusion': 'lack of fusion porosity'\n",
    "      }\n",
    "      return classes_dict.get(class_name)\n",
    "\n",
    "    '''\n",
    "    Ensures extracted row and column coords are not out of bounds\n",
    "    '''\n",
    "    def normalize_dimensions(self, col_min, col_max, row_min, row_max):\n",
    "      return max(col_min, 0), col_max, max(row_min, 0), row_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dTSnxV2w3NJb",
    "outputId": "2a170e81-2f4a-41a7-8da6-4a2790e2cbc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_12.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G0/A1G0COL_12_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_44.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G0/A1G0COL_44_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_315.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G0/A1G0COL_315_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_514.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G0/A1G0COL_514_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_43.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G9/A1G9COL_43_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_11.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G9/A1G9COL_11_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_29.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G9/A1G9COL_29_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_34.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G9/A1G9COL_34_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_12_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H0/A1H0COL_12_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_212_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H0/A1H0COL_212_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_45_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H0/A1H0COL_45_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_317_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H0/A1H0COL_317_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_117.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H4/A1H4COL_117.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_411.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H4/A1H4COL_411.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_56.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H4/A1H4COL_56.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_28.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H4/A1H4COL_28.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_311.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H5/A1H5COL_311.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_45.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H5/A1H5COL_45.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_216.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H5/A1H5COL_216.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_12.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H5/A1H5COL_12.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_218.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H6/A1H6COL_218_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_14.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H6/A1H6COL_14_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_514.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H6/A1H6COL_514_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_512.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H6/A1H6COL_512_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_55.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H8/A1H8COL_55_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_411.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H8/A1H8COL_411_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_212.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H8/A1H8COL_212_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_418.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H8/A1H8COL_418_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_514.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H9/A1H9COL_514.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_41.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H9/A1H9COL_41.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_315.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H9/A1H9COL_315.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_15.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H9/A1H9COL_15.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_58.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J0/A1J0COL_58_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_43.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J0/A1J0COL_43_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_11.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J0/A1J0COL_11_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_417.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J0/A1J0COL_417_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_47.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J3/A1J3COL_47_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_34.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J3/A1J3COL_34_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_210.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J3/A1J3COL_210_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_13.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J3/A1J3COL_13_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_14.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J4/AIJ4COL_14_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_57.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J4/AIJ4COL_57_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_317.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J4/AIJ4COL_317_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_42.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J4/AIJ4COL_42_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_15.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J7/A1J7COL_15_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_315.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J7/A1J7COL_315_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_212.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J7/A1J7COL_212_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_415.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J7/A1J7COL_415_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_42.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J8/A1J8COL_42_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_58.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J8/A1J8COL_58_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_13.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J8/A1J8COL_13_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_116.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J8/A1J8COL_116_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_43.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled K0/A1K0COL_43.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_27.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled K0/A1K0COL_27.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_315.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled K0/A1K0COL_315.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_211.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled K0/A1K0COL_211.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_14.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q0/A1Q0COL_14_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_316.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q0/A1Q0COL_316_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_29.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q0/A1Q0COL_29_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_59.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q0/A1Q0COL_59_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_58_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q3/A1Q3COL_58_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_45_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q3/A1Q3COL_45_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_510_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q3/A1Q3COL_510_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_12_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q3/A1Q3COL_12_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_46.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q5/A1Q5COL_46.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_115.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q5/A1Q5COL_115.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_13.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q5/A1Q5COL_13.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_31.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q5/A1Q5COL_31.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_12.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q9/A1Q9COL_12.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_56.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q9/A1Q9COL_56.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_415.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q9/A1Q9COL_415.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_211.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q9/A1Q9COL_211.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_15.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R2/A1R2COL_15.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_58.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R2/A1R2COL_58.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_214.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R2/A1R2COL_214.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_44.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R2/A1R2COL_44.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_58_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R6/A1R6COL_58_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_13_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R6/A1R6COL_13_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_417_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R6/A1R6COL_417_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_43_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R6/A1R6COL_43_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_110.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R7/A1R7COL_110_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_512.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R7/A1R7COL_512_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_417.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R7/A1R7COL_417_20X_YZ.json\n",
      "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_15.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R7/A1R7COL_15_20X_YZ.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
      "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-017aefad4859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmAP\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmAR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmAP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mAP: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmAP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mAR: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"numpy.float64\") to str"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_ar(pred_boxes, gt_boxes, list_iou_thresholds):\n",
    "    AR = []\n",
    "    for iou_threshold in list_iou_thresholds:\n",
    "        try:\n",
    "            recall, _ = compute_recall(pred_boxes, gt_boxes, iou=iou_threshold)\n",
    "            AR.append(recall)\n",
    "        except:\n",
    "            AR.append(0.0)\n",
    "            pass\n",
    "    AUC = 2 * (metrics.auc(list_iou_thresholds, AR))\n",
    "    return AUC\n",
    "\n",
    "    #Load dataset\n",
    "dataset_val = TestDataset()\n",
    "dataset_val.load_dataset()\n",
    "dataset_val.prepare()\n",
    "\n",
    "img_ids = dataset_val.image_ids\n",
    "\n",
    "#%%\n",
    "\n",
    "from mrcnn.utils import compute_ap\n",
    "from mrcnn.model import mold_image\n",
    "\n",
    "# make prediction & visualize\n",
    "APs = []\n",
    "ARs = []\n",
    "\n",
    "for img_id in img_ids:\n",
    "    img = dataset_val.load_image(img_id)\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset_val, cfg, img_id, use_mini_mask=False)\n",
    "\n",
    "    scaled_image = mold_image(image, cfg)\n",
    "\n",
    "    sample = np.expand_dims(scaled_image, 0)\n",
    "\n",
    "    yhat = model.detect(sample, verbose=0)\n",
    "\n",
    "    r = yhat[0]\n",
    "\n",
    "    AP, precisions, recalls, overlaps = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'], iou_threshold=0.5)\n",
    "    APs.append(AP)\n",
    "\n",
    "    list_iou_thresholds = np.arange(0.5, 1.01, 0.1)\n",
    "    AR = compute_ar(r['rois'], gt_bbox, list_iou_thresholds)\n",
    "    ARs.append(AR)\n",
    "\n",
    "\n",
    "mAP = np.mean(APs)\n",
    "mAR = np.mean(ARs)\n",
    "\n",
    "f1_score = 2 * ((mAP * mAR) / (mAP + mAR))\n",
    "\n",
    "print(\"mAP: \" + mAP)\n",
    "print(\"mAR: \" + mAR)\n",
    "print(\"F1: \" + f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrfXbrF62EAQ",
    "outputId": "ffe005bc-ca3f-4b37-f8eb-fb36b0e52cc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, nan, nan, nan, nan, 0.0, 0.10000000149011612, 0.08333333333333333, 0.20833333333333331, nan, nan, nan, 0.0, 0.0, 0.0, 0.0, 0.10000000149011612, 0.0, 0.20000000298023224, nan, 0.6000000238418579, 0.4000000059604645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.1666666716337204, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, 0.75, 0.29629629850387573, 0.0, 0.6666666865348816, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.3999999999999999, 0.12499999999999997, 0.3499999999999999, 0.3, 0.0, 0.0, 0.0, 0.0, 0.5666666666666667, 0.3, 0.29999999999999993, 0.32499999999999996, 0.0, 0.0, 0.0, 0.25999999999999995, 0.3999999999999999, 0.0, 0.4999999999999999, 0.41999999999999993, 0.17999999999999997, 0.09999999999999998, 0.0, 0.25999999999999995, 0.21999999999999997, 0.13999999999999999, 0.19999999999999996, 0.13999999999999996, 0.14999999999999997, 0.1555555555555555, 0.29999999999999993, 0.4999999999999999, 0.3499999999999999, 0.4999999999999999, 0.46666666666666656, 0.4999999999999999, 0.4999999999999999, 0.6999999999999998, 0.5666666666666665, 0.19999999999999996, 0.29999999999999993, 0.6999999999999998, 0.24999999999999994, 0.31428571428571417, 0.4999999999999999, 0.3999999999999999, 0.6999999999999998, 0.4999999999999999, 0.3999999999999999, 0.19999999999999993, 0.09999999999999998, 0.3499999999999999, 0.4999999999999999, 0.0, 0.14999999999999997, 0.24999999999999994, 0.04999999999999999, 0.4999999999999999, 0.3499999999999999, 0.0, 0.29999999999999993, 0.6999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6499999999999999, 0.21111111111111105, 0.4999999999999999, 0.6999999999999998, 0.3166666666666666, 0.4999999999999999, 0.12499999999999997, 0.3499999999999999]\n",
      "0.14227897353390212\n",
      "0.2723412698412698\n",
      "['G0/A1G0COL_12.tif', 'G0/A1G0COL_44.tif', 'G0/A1G0COL_315.tif', 'G0/A1G0COL_514.tif', 'G9/A1G9COL_43.tif', 'G9/A1G9COL_11.tif', 'G9/A1G9COL_29.tif', 'G9/A1G9COL_34.tif', 'H0/A1H0COL_12_20X_YZ.tif', 'H0/A1H0COL_212_20X_YZ.tif', 'H0/A1H0COL_45_20X_YZ.tif', 'H0/A1H0COL_317_20X_YZ.tif', 'H4/A1H4COL_411.tif', 'H4/A1H4COL_56.tif', 'H4/A1H4COL_28.tif', 'H5/A1H5COL_311.tif', 'H5/A1H5COL_45.tif', 'H5/A1H5COL_216.tif', 'H5/A1H5COL_12.tif', 'H6/A1H6COL_218.tif', 'H6/A1H6COL_14.tif', 'H6/A1H6COL_514.tif', 'H6/A1H6COL_512.tif', 'H8/A1H8COL_55.tif', 'H8/A1H8COL_411.tif', 'H8/A1H8COL_212.tif', 'H8/A1H8COL_418.tif', 'H9/A1H9COL_514.tif', 'H9/A1H9COL_41.tif', 'H9/A1H9COL_315.tif', 'H9/A1H9COL_15.tif', 'J0/A1J0COL_58.tif', 'J0/A1J0COL_43.tif', 'J0/A1J0COL_11.tif', 'J3/A1J3COL_47.tif', 'J3/A1J3COL_34.tif', 'J3/A1J3COL_210.tif', 'J3/A1J3COL_13.tif', 'J4/AIJ4COL_14.tif', 'J4/AIJ4COL_57.tif', 'J4/AIJ4COL_317.tif', 'J4/AIJ4COL_42.tif', 'J7/A1J7COL_15.tif', 'J7/A1J7COL_315.tif', 'J7/A1J7COL_212.tif', 'J7/A1J7COL_415.tif', 'J8/A1J8COL_42.tif', 'J8/A1J8COL_13.tif', 'J8/A1J8COL_116.tif', 'K0/A1K0COL_43.tif', 'K0/A1K0COL_27.tif', 'K0/A1K0COL_315.tif', 'K0/A1K0COL_211.tif', 'Q0/A1Q0COL_14.tif', 'Q0/A1Q0COL_316.tif', 'Q0/A1Q0COL_29.tif', 'Q0/A1Q0COL_59.tif', 'Q3/A1Q3COL_45_20X_YZ.tif', 'Q3/A1Q3COL_510_20X_YZ.tif', 'Q3/A1Q3COL_12_20X_YZ.tif', 'Q5/A1Q5COL_46.tif', 'Q5/A1Q5COL_31.tif', 'Q9/A1Q9COL_12.tif', 'Q9/A1Q9COL_56.tif', 'Q9/A1Q9COL_415.tif', 'Q9/A1Q9COL_211.tif', 'R2/A1R2COL_15.tif', 'R2/A1R2COL_58.tif', 'R2/A1R2COL_214.tif', 'R2/A1R2COL_44.tif', 'R6/A1R6COL_58_20X_YZ.tif', 'R6/A1R6COL_13_20X_YZ.tif', 'R6/A1R6COL_417_20X_YZ.tif', 'R6/A1R6COL_43_20X_YZ.tif', 'R7/A1R7COL_110.tif', 'R7/A1R7COL_512.tif', 'R7/A1R7COL_417.tif', 'R7/A1R7COL_15.tif']\n",
      "defaultdict(<class 'list'>, {'G0': [0.0, 0.0, 0.0, 0.0], 'G9': [nan, nan, nan, nan], 'H0': [0.0, 0.10000000149011612, 0.08333333333333333, 0.20833333333333331], 'H4': [nan, nan, nan], 'H5': [0.0, 0.0, 0.0, 0.0], 'H6': [0.10000000149011612, 0.0, 0.20000000298023224, nan], 'H8': [0.6000000238418579, 0.4000000059604645, 0.0, 0.0], 'H9': [0.0, 0.0, 0.0, 0.0], 'J0': [0.5, 0.0, 0.0], 'J3': [0.1666666716337204, 0.0, 0.0, 0.0], 'J4': [0.0, 0.25, 0.0, 1.0], 'J7': [0.5, 0.0, 0.0, 0.0], 'J8': [1.0, 1.0, 0.0], 'K0': [0.0, 0.0, 0.0, 0.0], 'Q0': [0.0, 0.5, 0.0, 0.0], 'Q3': [0.0, 0.5, 0.0], 'Q5': [0.0, 0.0], 'Q9': [nan, nan, nan, nan], 'R2': [nan, nan, nan, nan], 'R6': [0.75, 0.29629629850387573, 0.0, 0.6666666865348816], 'R7': [0.0, 0.0, 0.0, 0.0]})\n",
      "defaultdict(<class 'list'>, {'G0': [0.3999999999999999, 0.12499999999999997, 0.3499999999999999, 0.3], 'G9': [0.0, 0.0, 0.0, 0.0], 'H0': [0.5666666666666667, 0.3, 0.29999999999999993, 0.32499999999999996], 'H4': [0.0, 0.0, 0.0], 'H5': [0.25999999999999995, 0.3999999999999999, 0.0, 0.4999999999999999], 'H6': [0.41999999999999993, 0.17999999999999997, 0.09999999999999998, 0.0], 'H8': [0.25999999999999995, 0.21999999999999997, 0.13999999999999999, 0.19999999999999996], 'H9': [0.13999999999999996, 0.14999999999999997, 0.1555555555555555, 0.29999999999999993], 'J0': [0.4999999999999999, 0.3499999999999999, 0.4999999999999999], 'J3': [0.46666666666666656, 0.4999999999999999, 0.4999999999999999, 0.6999999999999998], 'J4': [0.5666666666666665, 0.19999999999999996, 0.29999999999999993, 0.6999999999999998], 'J7': [0.24999999999999994, 0.31428571428571417, 0.4999999999999999, 0.3999999999999999], 'J8': [0.6999999999999998, 0.4999999999999999, 0.3999999999999999], 'K0': [0.19999999999999993, 0.09999999999999998, 0.3499999999999999, 0.4999999999999999], 'Q0': [0.0, 0.14999999999999997, 0.24999999999999994, 0.04999999999999999], 'Q3': [0.4999999999999999, 0.3499999999999999, 0.0], 'Q5': [0.29999999999999993, 0.6999999999999998], 'Q9': [0.0, 0.0, 0.0, 0.0], 'R2': [0.0, 0.0, 0.0, 0.0], 'R6': [0.6499999999999999, 0.21111111111111105, 0.4999999999999999, 0.6999999999999998], 'R7': [0.3166666666666666, 0.4999999999999999, 0.12499999999999997, 0.3499999999999999]})\n",
      "{'G0': 0.0, 'G9': nan, 'H0': 0.09791666703919569, 'H4': nan, 'H5': 0.0, 'H6': nan, 'H8': 0.2500000074505806, 'H9': 0.0, 'J0': 0.16666666666666666, 'J3': 0.0416666679084301, 'J4': 0.3125, 'J7': 0.125, 'J8': 0.6666666666666666, 'K0': 0.0, 'Q0': 0.125, 'Q3': 0.16666666666666666, 'Q5': 0.0, 'Q9': nan, 'R2': nan, 'R6': 0.42824074625968933, 'R7': 0.0}\n",
      "{'G0': 0.29374999999999996, 'G9': 0.0, 'H0': 0.3729166666666666, 'H4': 0.0, 'H5': 0.2899999999999999, 'H6': 0.17499999999999996, 'H8': 0.20499999999999996, 'H9': 0.18638888888888883, 'J0': 0.4499999999999999, 'J3': 0.5416666666666665, 'J4': 0.44166666666666654, 'J7': 0.3660714285714285, 'J8': 0.5333333333333332, 'K0': 0.2875, 'Q0': 0.11249999999999998, 'Q3': 0.28333333333333327, 'Q5': 0.4999999999999999, 'Q9': 0.0, 'R2': 0.0, 'R6': 0.5152777777777776, 'R7': 0.3229166666666666}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pprint\n",
    "\n",
    "\n",
    "print(APs)\n",
    "print(ARs)\n",
    "newAPs = [x for x in APs if np.isnan(x) == False]\n",
    "mAp_new = np.mean(newAPs)\n",
    "print(mAp_new)\n",
    "print(np.mean(ARs))\n",
    "len(APs)\n",
    "print(dataset_val.image_list)\n",
    "dict_AP = defaultdict(list)\n",
    "dict_AR = defaultdict(list)\n",
    "for idx, img in enumerate(dataset_val.image_list):\n",
    "  folder = img.split(\"/\")[0]\n",
    "  dict_AP[folder].append(APs[idx])\n",
    "  dict_AR[folder].append(ARs[idx])\n",
    "\n",
    "print(dict_AP)\n",
    "print(dict_AR)\n",
    "\n",
    "dict_mAP = {}\n",
    "dict_mAR = {}\n",
    "\n",
    "for key in dict_AP.keys():\n",
    "    dict_mAP[key] = np.mean(dict_AP[key])\n",
    "\n",
    "for key in dict_AR.keys():\n",
    "    dict_mAR[key] = np.mean(dict_AR[key])\n",
    "\n",
    "print(dict_mAP)\n",
    "print(dict_mAR)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOJYn/0qOaLVVBWY330YHxs",
   "include_colab_link": true,
   "name": "Model evaluation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
