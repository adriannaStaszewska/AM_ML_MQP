{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model evaluation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPm3B/HLTh4+izDl/4WtTzc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cabroderick/ML-AM-MQP/blob/main/Model_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python --version\n",
        "\n",
        "# uninstall improper package versions\n",
        "!pip uninstall keras -y\n",
        "!pip uninstall keras-nightly -y\n",
        "!pip uninstall keras-Preprocessing -y\n",
        "!pip uninstall keras-vis -y\n",
        "!pip uninstall tensorflow -y\n",
        "!pip uninstall h5py -y\n",
        "\n",
        "# reinstall with proper versions\n",
        "!pip install tensorflow==1.13.1\n",
        "!pip install keras==2.0.8\n",
        "!pip install h5py==2.10.0\n",
        "\n",
        "# import mask rcnn and set up\n",
        "%cd\n",
        "!git clone https://github.com/matterport/Mask_RCNN.git\n",
        "%cd Mask_RCNN/\n",
        "!python setup.py install\n",
        "!pip show mask-rcnn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3amtZgDq2EN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.model import MaskRCNN\n",
        "from mrcnn.model import load_image_gt\n",
        "from mrcnn import utils\n",
        "from sklearn import metrics\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Rectangle\n",
        "import cv2\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from mrcnn.utils import compute_recall\n",
        "\n",
        "#%%\n",
        "\n",
        "# specify paths for prediction\n",
        "WEIGHTS_PATH = '/content/drive/MyDrive/MQP/custom_mrcnn.h5'\n",
        "TEST_IMGS = []\n",
        "CLASS_NAMES = ['gas entrapment porosity', 'lack of fusion porosity', 'keyhole porosity']"
      ],
      "metadata": {
        "id": "EWfzAaVh25sU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# configure inference model\n",
        "class InferenceConfig(Config):\n",
        "    NAME = 'inference'\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    NUM_CLASSES = 4\n",
        "\n",
        "cfg = InferenceConfig()\n",
        "\n",
        "model = MaskRCNN(mode='inference',\n",
        "                 config=cfg, model_dir='./')\n",
        "\n",
        "model.load_weights(filepath=WEIGHTS_PATH, by_name=True)\n"
      ],
      "metadata": {
        "id": "AWoF-fr83B1A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TestDataset(utils.Dataset):\n",
        "\n",
        "    # define constants\n",
        "    BASE_IMAGES_DIR = '/content/drive/MyDrive/MQP/Data/Eval/Images/' # directory where all images can be found\n",
        "    BASE_ANNOTATIONS_DIR = '/content/drive/MyDrive/MQP/Data/Eval/Labels/' # directory where all images labels can be found\n",
        "    IMAGES_DIRS = ['G0/', 'G9/', 'H0/', 'H4/', 'H5/', 'H6/', 'H8/', 'H9/', 'J0/', 'J3/', 'J4/', 'J7/',\n",
        "                 'J8/', 'K0/', 'Q0/', 'Q3/', 'Q5/', 'Q9/', 'R2/', 'R6/', 'R7/'] # list of directories where images are contained\n",
        "    ANNOTATIONS_DIRS = ['Labeled G0/', 'Labeled G9/', 'Labeled H0/', 'Labeled H4/', 'Labeled H5/', 'Labeled H6/',\n",
        "                      'Labeled H8/', 'Labeled H9/', 'Labeled J0/', 'Labeled J3/', 'Labeled J4/',\n",
        "                      'Labeled J7/', 'Labeled J8/', 'Labeled K0/', 'Labeled Q0/', 'Labeled Q3/', 'Labeled Q5/',\n",
        "                      'Labeled Q9/', 'Labeled R2/', 'Labeled R6/', 'Labeled R7/'] # corresponding list of directories where annotations are contained\n",
        "\n",
        "    CLASSES = ['lack of fusion porosity', 'keyhole porosity', 'other'] # all annotation classes\n",
        "\n",
        "    def load_dataset(self):\n",
        "        image_paths = []\n",
        "        annotation_paths = []\n",
        "        image_ids = []\n",
        "\n",
        "        for i in range(len(self.IMAGES_DIRS)):\n",
        "            image_paths.append([])\n",
        "            annotation_paths.append([])\n",
        "            image_ids.append([])\n",
        "            i_dir = self.BASE_IMAGES_DIR + self.IMAGES_DIRS[i]\n",
        "            a_dir = self.BASE_ANNOTATIONS_DIR + self.ANNOTATIONS_DIRS[i]\n",
        "            for file in os.listdir(i_dir):\n",
        "                i_id = file[:-4]\n",
        "                image_ids[i].append(i_id)\n",
        "                image_paths[i].append(i_dir+i_id+'.tif')\n",
        "                if \"20X_YZ\" not in i_id:\n",
        "                  annotation_paths[i].append(a_dir+i_id+'_20X_YZ.json')\n",
        "                else:\n",
        "                  annotation_paths[i].append(a_dir+i_id+'.json')\n",
        "\n",
        "        if (len(image_paths) != len(annotation_paths)): # raise exception if mismatch betwaeen number of images and annotations\n",
        "            raise(ValueError('Number of images and annotations must be equal'))\n",
        "        # configure dataset\n",
        "        for i in range(len(self.CLASSES)):\n",
        "            self.add_class('dataset', i+1, self.CLASSES[i]) # add classes to model\n",
        "\n",
        "        # add images and annotations to dataset, ensuring an even distribution\n",
        "        for i in range(len(image_paths)):\n",
        "            images = len(image_paths[i])\n",
        "            for j in range(images):\n",
        "              image_id = image_ids[i][j]\n",
        "              image_path = image_paths[i][j]\n",
        "              annotation_path = annotation_paths[i][j]\n",
        "\n",
        "              mask, class_ids = self.extract_mask(image_path, annotation_path)\n",
        "              print(image_path)\n",
        "\n",
        "              self.add_image('dataset',\n",
        "                                    image_id=image_id,\n",
        "                                    path=image_path,\n",
        "                                    mask=mask,\n",
        "                                    class_ids=class_ids)\n",
        "    '''\n",
        "    Extracts a mask from an image\n",
        "    image_id: The image id to extract the mask from\n",
        "    Returns a mask and a corresponding list of class ids\n",
        "    '''\n",
        "    def load_mask(self, image_id):\n",
        "\n",
        "        info = self.image_info[image_id] # extract image info from data added earlier\n",
        "        mask = info['mask']\n",
        "        class_ids = info['class_ids']\n",
        "\n",
        "        return mask, class_ids\n",
        "\n",
        "    def extract_mask(self, image_path, annotation_path):\n",
        "      if not os.path.exists(annotation_path): # if the annotation path is not found, it is named differently than its source image\n",
        "          annotation_path = annotation_path[:-5] + '_20X_YZ.json'\n",
        "      if not os.path.exists(annotation_path): # if the annotation path is not found, it is named differently than its source image\n",
        "          annotation_path = annotation_path.replace('_20X_YZ', \"\")\n",
        "\n",
        "      print(image_path, annotation_path)\n",
        "\n",
        "      f_ann = open(annotation_path,)\n",
        "      annotation_json = json.load(f_ann)\n",
        "\n",
        "      if not annotation_json['shapes']: # if there are no annotations to be extracted\n",
        "          return [], [] # empty list return values will be ignored and thus image is ignored\n",
        "\n",
        "      class_ids = []\n",
        "      image = cv2.imread(image_path)\n",
        "      height = image.shape[0]\n",
        "      width = image.shape[1]\n",
        "\n",
        "      annotation_list = []\n",
        "      [annotation_list.append(shape) for shape in annotation_json['shapes'] if shape['shape_type'] =='rectangle'\n",
        "      and self.normalize_classname(shape['label']) != 'gas entrapment porosity'] # get annotations in a list\n",
        "      mask = np.zeros([height, width, len(annotation_list)], dtype='uint8') # initialize array of masks for each bounding box\n",
        "\n",
        "      for i in range(len(annotation_list)):\n",
        "        a = annotation_list[i]\n",
        "\n",
        "        # extract row and col data and crop image to annotation size\n",
        "        col_min, col_max = int(min(a['points'][0][0], a['points'][1][0])), int(max(a['points'][0][0], a['points'][1][0]))\n",
        "        row_min, row_max = int(min(a['points'][0][1], a['points'][1][1])), int(max(a['points'][0][1], a['points'][1][1]))\n",
        "        col_min, col_max, row_min, row_max = self.normalize_dimensions(col_min, col_max, row_min, row_max)\n",
        "        cropped_img = image[row_min:row_max, col_min:col_max]  # crop image to size of bounding box\n",
        "        cropped_img_gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n",
        "        edged = cv2.Canny(cropped_img_gray, 30, 200)\n",
        "\n",
        "        # apply contour to image and fill\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
        "        dilated = cv2.dilate(edged, kernel)\n",
        "        contours, hierarchy = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        polygon = np.zeros(cropped_img.shape)\n",
        "        color = [255, 255, 255]\n",
        "        cv2.fillPoly(polygon, contours, color)\n",
        "\n",
        "        # normalize polygon to all boolean values and insert into mask\n",
        "        polygon_bool = np.alltrue(polygon == color, axis=2)\n",
        "        mask[row_min:row_max, col_min:col_max, i] = polygon_bool\n",
        "\n",
        "        # draw contour and mask\n",
        "        # cv2.drawContours(edged, contours, -1, (0, 255, 0), 1)\n",
        "        # imS = cv2.resize(edged, (512, 512))\n",
        "        # cv2.imshow('Contours', imS)\n",
        "        # cv2.waitKey(0)\n",
        "        # cv2.imshow('Polygon', cv2.resize(polygon, (512, 512)))\n",
        "        # cv2.waitKey(0)\n",
        "\n",
        "        # extract class id and append to list\n",
        "        class_label = self.normalize_classname(a['label'])\n",
        "        class_id = self.CLASSES.index(class_label)\n",
        "        class_ids.append(class_id)\n",
        "\n",
        "      return mask.astype(np.bool), np.array(class_ids, dtype=np.int32)\n",
        "\n",
        "    def normalize_classname(self, class_name): # normalize the class name to one used by the model\n",
        "      class_name = class_name.lower() # remove capitalization\n",
        "      class_name = class_name.strip() # remove leading and trailing whitespace\n",
        "      classes_dict = { # dictionary containing all class names used in labels and their appropriate model class name\n",
        "        'gas entrapment porosity': 'gas entrapment porosity',\n",
        "        'keyhole porosity': 'keyhole porosity',\n",
        "        'lack of fusion porosity': 'lack of fusion porosity',\n",
        "        'fusion porosity': 'lack of fusion porosity',\n",
        "        'gas porosity': 'gas entrapment porosity',\n",
        "        'lack-of-fusion': 'lack of fusion porosity',\n",
        "        'keyhole': 'keyhole porosity',\n",
        "        'other': 'other',\n",
        "        'lack of fusion': 'lack of fusion porosity'\n",
        "      }\n",
        "      return classes_dict.get(class_name)\n",
        "\n",
        "    '''\n",
        "    Ensures extracted row and column coords are not out of bounds\n",
        "    '''\n",
        "    def normalize_dimensions(self, col_min, col_max, row_min, row_max):\n",
        "      return max(col_min, 0), col_max, max(row_min, 0), row_max"
      ],
      "metadata": {
        "id": "2IsIR1te3bwq"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_ar(pred_boxes, gt_boxes, list_iou_thresholds):\n",
        "    AR = []\n",
        "    for iou_threshold in list_iou_thresholds:\n",
        "        try:\n",
        "            recall, _ = compute_recall(pred_boxes, gt_boxes, iou=iou_threshold)\n",
        "            AR.append(recall)\n",
        "        except:\n",
        "            AR.append(0.0)\n",
        "            pass\n",
        "    AUC = 2 * (metrics.auc(list_iou_thresholds, AR))\n",
        "    return AUC\n",
        "\n",
        "    #Load dataset\n",
        "dataset_val = TestDataset()\n",
        "dataset_val.load_dataset()\n",
        "dataset_val.prepare()\n",
        "\n",
        "img_ids = dataset_val.image_ids\n",
        "\n",
        "#%%\n",
        "\n",
        "from mrcnn.utils import compute_ap\n",
        "from mrcnn.model import mold_image\n",
        "\n",
        "# make prediction & visualize\n",
        "APs = []\n",
        "ARs = []\n",
        "\n",
        "for img_id in img_ids:\n",
        "    print(img_id)\n",
        "    print(APs)\n",
        "\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset_val, cfg, img_id, use_mini_mask=False)\n",
        "\n",
        "    scaled_image = mold_image(image, cfg)\n",
        "\n",
        "    sample = np.expand_dims(scaled_image, 0)\n",
        "\n",
        "    yhat = model.detect(sample, verbose=0)\n",
        "\n",
        "    r = yhat[0]\n",
        "\n",
        "    AP, precisions, recalls, overlaps = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'], iou_threshold=0.5)\n",
        "    APs.append(AP)\n",
        "\n",
        "    list_iou_thresholds = np.arange(0.5, 1.01, 0.1)\n",
        "    AR = compute_ar(r['rois'], gt_bbox, list_iou_thresholds)\n",
        "    ARs.append(AR)\n",
        "\n",
        "\n",
        "mAP = np.mean(APs)\n",
        "mAR = np.mean(ARs)\n",
        "\n",
        "f1_score = 2 * ((mAP * mAR) / (mAP + mAR))\n",
        "\n",
        "print(\"mAP: \" + mAP)\n",
        "print(\"mAR: \" + mAR)\n",
        "print(\"F1: \" + f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dTSnxV2w3NJb",
        "outputId": "d2dc03d0-9c44-4f58-9437-c554daced9b7"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_12.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G0/A1G0COL_12_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_12.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_315.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G0/A1G0COL_315_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_315.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_44.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G0/A1G0COL_44_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_44.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_514.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G0/A1G0COL_514_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G0/A1G0COL_514.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_43.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G9/A1G9COL_43_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_43.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_11.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G9/A1G9COL_11_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_11.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_29.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G9/A1G9COL_29_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_29.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_34.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled G9/A1G9COL_34_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/G9/A1G9COL_34.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_12_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H0/A1H0COL_12_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_12_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_212_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H0/A1H0COL_212_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_212_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_45_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H0/A1H0COL_45_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_45_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_317_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H0/A1H0COL_317_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H0/A1H0COL_317_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_117.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H4/A1H4COL_117.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_117.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_411.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H4/A1H4COL_411.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_411.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_56.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H4/A1H4COL_56.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_56.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_28.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H4/A1H4COL_28.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H4/A1H4COL_28.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_311.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H5/A1H5COL_311.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_311.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_45.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H5/A1H5COL_45.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_45.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_216.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H5/A1H5COL_216.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_216.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_12.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H5/A1H5COL_12.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H5/A1H5COL_12.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_218.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H6/A1H6COL_218_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_218.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_14.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H6/A1H6COL_14_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_14.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_514.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H6/A1H6COL_514_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_514.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_512.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H6/A1H6COL_512_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H6/A1H6COL_512.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_55.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H8/A1H8COL_55_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_55.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_411.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H8/A1H8COL_411_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_411.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_212.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H8/A1H8COL_212_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_212.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_418.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H8/A1H8COL_418_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H8/A1H8COL_418.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_514.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H9/A1H9COL_514.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_514.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_41.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H9/A1H9COL_41.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_41.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_315.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H9/A1H9COL_315.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_315.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_15.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled H9/A1H9COL_15.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/H9/A1H9COL_15.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_58.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J0/A1J0COL_58_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_58.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_43.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J0/A1J0COL_43_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_43.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_11.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J0/A1J0COL_11_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_11.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_417.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J0/A1J0COL_417_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J0/A1J0COL_417.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_47.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J3/A1J3COL_47_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_47.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_34.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J3/A1J3COL_34_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_34.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_210.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J3/A1J3COL_210_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_210.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_13.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J3/A1J3COL_13_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J3/A1J3COL_13.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_14.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J4/AIJ4COL_14_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_14.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_57.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J4/AIJ4COL_57_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_57.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_317.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J4/AIJ4COL_317_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_317.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_42.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J4/AIJ4COL_42_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J4/AIJ4COL_42.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_15.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J7/A1J7COL_15_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_15.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_315.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J7/A1J7COL_315_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_315.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_212.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J7/A1J7COL_212_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_212.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_415.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J7/A1J7COL_415_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J7/A1J7COL_415.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_42.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J8/A1J8COL_42_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_42.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_58.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J8/A1J8COL_58_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_58.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_13.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J8/A1J8COL_13_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_13.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_116.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled J8/A1J8COL_116_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/J8/A1J8COL_116.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_43.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled K0/A1K0COL_43.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_43.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_27.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled K0/A1K0COL_27.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_27.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_315.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled K0/A1K0COL_315.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_315.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_211.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled K0/A1K0COL_211.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/K0/A1K0COL_211.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_14.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q0/A1Q0COL_14_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_14.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_316.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q0/A1Q0COL_316_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_316.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_29.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q0/A1Q0COL_29_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_29.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_59.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q0/A1Q0COL_59_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q0/A1Q0COL_59.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_58_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q3/A1Q3COL_58_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_58_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_45_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q3/A1Q3COL_45_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_45_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_510_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q3/A1Q3COL_510_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_510_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_12_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q3/A1Q3COL_12_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q3/A1Q3COL_12_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_46.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q5/A1Q5COL_46.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_46.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_115.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q5/A1Q5COL_115.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_115.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_13.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q5/A1Q5COL_13.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_13.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_31.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q5/A1Q5COL_31.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q5/A1Q5COL_31.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_12.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q9/A1Q9COL_12.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_12.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_56.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q9/A1Q9COL_56.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_56.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_415.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q9/A1Q9COL_415.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_415.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_211.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled Q9/A1Q9COL_211.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/Q9/A1Q9COL_211.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_15.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R2/A1R2COL_15.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_15.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_58.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R2/A1R2COL_58.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_58.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_214.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R2/A1R2COL_214.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_214.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_44.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R2/A1R2COL_44.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R2/A1R2COL_44.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_58_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R6/A1R6COL_58_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_58_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_13_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R6/A1R6COL_13_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_13_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_417_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R6/A1R6COL_417_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_417_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_43_20X_YZ.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R6/A1R6COL_43_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R6/A1R6COL_43_20X_YZ.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_110.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R7/A1R7COL_110_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_110.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_512.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R7/A1R7COL_512_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_512.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_417.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R7/A1R7COL_417_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_417.tif\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_15.tif /content/drive/MyDrive/MQP/Data/Eval/Labels/Labeled R7/A1R7COL_15_20X_YZ.json\n",
            "/content/drive/MyDrive/MQP/Data/Eval/Images/R7/A1R7COL_15.tif\n",
            "0\n",
            "[]\n",
            "1\n",
            "[0.0]\n",
            "2\n",
            "[0.0, 0.0]\n",
            "3\n",
            "[0.0, 0.0, 0.0]\n",
            "4\n",
            "[0.0, 0.0, 0.0, 0.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "[0.0, 0.0, 0.0, 0.0, nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "[0.0, 0.0, 0.0, 0.0, nan, nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "[0.0, 0.0, 0.0, 0.0, nan, nan, nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/Mask_RCNN/mrcnn/utils.py:734: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "[0.0, 0.0, 0.0, 0.0, nan, nan, nan, nan]\n",
            "9\n",
            "[0.0, 0.0, 0.0, 0.0, nan, nan, nan, nan, 0.0]\n",
            "10\n",
            "[0.0, 0.0, 0.0, 0.0, nan, nan, nan, nan, 0.0, 0.10000000149011612]\n",
            "11\n",
            "[0.0, 0.0, 0.0, 0.0, nan, nan, nan, nan, 0.0, 0.10000000149011612, 0.08333333333333333]\n",
            "12\n",
            "[0.0, 0.0, 0.0, 0.0, nan, nan, nan, nan, 0.0, 0.10000000149011612, 0.08333333333333333, 0.20833333333333331]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-a837360f140f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_class_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image_gt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_mini_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mscaled_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmold_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mload_image_gt\u001b[0;34m(dataset, config, image_id, augment, augmentation, use_mini_mask)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0mmax_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE_MAX_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         mode=config.IMAGE_RESIZE_MODE)\n\u001b[0;32m-> 1220\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# Random horizontal flips.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/Mask_RCNN/mrcnn/utils.py\u001b[0m in \u001b[0;36mresize_mask\u001b[0;34m(mask, scale, padding, crop)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcrop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36mzoom\u001b[0;34m(input, zoom, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m     \u001b[0mzoom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     output_shape = tuple(\n\u001b[1;32m    603\u001b[0m             [int(round(ii * jj)) for ii, jj in zip(input.shape, zoom)])\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/ndimage/_ni_support.py\u001b[0m in \u001b[0;36m_normalize_sequence\u001b[0;34m(input, rank)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sequence argument must have length equal to input rank\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: sequence argument must have length equal to input rank"
          ]
        }
      ]
    }
  ]
}